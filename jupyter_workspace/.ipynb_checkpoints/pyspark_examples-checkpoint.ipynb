{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Examples\n",
    "This notebook demonstrates common PySpark operations and transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Examples\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample employee DataFrame\n",
    "employees_data = [\n",
    "    (1, \"John Doe\", 30, \"Engineering\", 75000),\n",
    "    (2, \"Jane Smith\", 25, \"Marketing\", 65000),\n",
    "    (3, \"Bob Johnson\", 35, \"Engineering\", 85000),\n",
    "    (4, \"Alice Brown\", 28, \"Marketing\", 67000),\n",
    "    (5, \"Charlie Wilson\", 40, \"Engineering\", 95000)\n",
    "]\n",
    "\n",
    "employees_df = spark.createDataFrame(employees_data, \n",
    "    [\"id\", \"name\", \"age\", \"department\", \"salary\"])\n",
    "\n",
    "# Create a sample departments DataFrame\n",
    "departments_data = [\n",
    "    (\"Engineering\", \"New York\"),\n",
    "    (\"Marketing\", \"San Francisco\"),\n",
    "    (\"Sales\", \"Chicago\")\n",
    "]\n",
    "\n",
    "departments_df = spark.createDataFrame(departments_data, \n",
    "    [\"department\", \"location\"])\n",
    "\n",
    "# Show the DataFrames\n",
    "print(\"Employees DataFrame:\")\n",
    "employees_df.show()\n",
    "\n",
    "print(\"\\nDepartments DataFrame:\")\n",
    "departments_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, sum, count\n",
    "\n",
    "# Select specific columns\n",
    "print(\"Select name and salary:\")\n",
    "employees_df.select(\"name\", \"salary\").show()\n",
    "\n",
    "# Filter data\n",
    "print(\"\\nEmployees with salary > 70000:\")\n",
    "employees_df.filter(col(\"salary\") > 70000).show()\n",
    "\n",
    "# Group by and aggregate\n",
    "print(\"\\nAverage salary by department:\")\n",
    "employees_df.groupBy(\"department\") \\\n",
    "    .agg(avg(\"salary\").alias(\"avg_salary\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Joining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join employees with departments\n",
    "print(\"Join employees with their department locations:\")\n",
    "joined_df = employees_df.join(departments_df, \"department\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary views\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "departments_df.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "# Run SQL query\n",
    "query = \"\"\"\n",
    "SELECT e.name, e.department, e.salary, d.location\n",
    "FROM employees e\n",
    "JOIN departments d ON e.department = d.department\n",
    "WHERE e.salary > 70000\n",
    "ORDER BY e.salary DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"SQL query result:\")\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Converting to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas\n",
    "pandas_df = employees_df.toPandas()\n",
    "print(\"Pandas DataFrame:\")\n",
    "print(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
